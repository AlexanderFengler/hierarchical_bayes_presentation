{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pystan\n",
    "import pickle\n",
    "from hashlib import md5\n",
    "import numpy as np\n",
    "\n",
    "# to run the model run the main() function like shown below\n",
    "# run: test = main('hierarchical_normal_data.txt', 'hierarchical_model.txt', 5000, 10)\n",
    "# note: use help(test) to get information about the file you generated\n",
    "\n",
    "# relevant filenames: model_code --> hierarchical_model.txt, data --> hierarchical_normal_data.txt\n",
    "def read_in_data(file_name):\n",
    "    data = dict()\n",
    "    with open(file_name) as f:\n",
    "        content = f.readlines()\n",
    "    lines = [x.rstrip('\\n') for x in content]\n",
    "    for i in range(len(lines)):\n",
    "        lines[i] = lines[i].split()\n",
    "        if i <= 1:\n",
    "            lines[i][1:] = [int(x) for x in lines[i][1:]]\n",
    "            data[lines[i][0]] = lines[i][1:]\n",
    "        else:\n",
    "            lines[i][1:] = [float(x) for x in lines[i][1:]]\n",
    "            data[lines[i][0]] = lines[i][1:]\n",
    "    return data\n",
    "\n",
    "def read_in_model_code(file_name):\n",
    "    # read in model code\n",
    "    f = open(file_name, 'r+')\n",
    "    model_code = f.read()\n",
    "    return model_code\n",
    "\n",
    "def StanModel_cache(model_code, model_name=None, **kwargs):\n",
    "    \"\"\"Use just as you would `stan`\"\"\"\n",
    "    # this function makes sure to reuse models so we don't have to recompile c everytime\n",
    "    code_hash = md5(model_code.encode('ascii')).hexdigest() # give model some id derived from model code\n",
    "    if model_name is None:\n",
    "        cache_fn = 'cached-model-{}.pkl'.format(code_hash)\n",
    "    else:\n",
    "        cache_fn = 'cached-{}-{}.pkl'.format(model_name, code_hash)\n",
    "    try:\n",
    "        sm = pickle.load(open(cache_fn, 'rb')) # try to load model if it has been stored already\n",
    "    except:\n",
    "        sm = pystan.StanModel(model_code=model_code) # if model has not been stored compile it\n",
    "        with open(cache_fn, 'wb') as f: # and save it for future use\n",
    "            pickle.dump(sm, f)\n",
    "    else:\n",
    "        print(\"Using cached StanModel\") # if we reused the model print some message\n",
    "    return sm\n",
    "\n",
    "\n",
    "def initialize_model(model_code):\n",
    "    sm = StanModel_cache(model_code = model_code)\n",
    "    return sm\n",
    "\n",
    "def run_model(stan_model, data, n_iter, n_chains, n_warmup):\n",
    "    fit = stan_model.sampling(data = data, iter = n_iter, chains = n_chains, warmup = n_warmup)\n",
    "    return fit\n",
    "\n",
    "def write_samples_to_csv(fit_object):\n",
    "    # create path if it doesn't exist\n",
    "    mypath = 'output'\n",
    "    if not os.path.isdir(mypath):\n",
    "        os.makedirs(mypath)\n",
    "\n",
    "    # get first line of csv file as string (headers)\n",
    "    fitdict = fit_object.extract(permuted = False)\n",
    "    my_str = ''\n",
    "    for i in range(len(fit_object.sim['fnames_oi'][:-1])):\n",
    "        my_str += fit_object.sim['fnames_oi'][i] + ', '\n",
    "\n",
    "    my_str = my_str.rstrip(', ')\n",
    "    my_str += ' \\n'\n",
    "\n",
    "    # write samples to csv\n",
    "    for i in range(np.shape(fitdict)[1]):\n",
    "        with open('output/chain_' + str(i + 1) + '.csv', 'wb') as f:\n",
    "            f.write(str.encode(my_str))\n",
    "            np.savetxt(f, fitdict[:,i,:-1], delimiter=\",\", fmt = '%1.8f')\n",
    "    return\n",
    "\n",
    "def main(data_file, model_file, n_iter, n_chains, n_warmup):\n",
    "    # full run of the model with output in csv file named chain_[*].csv\n",
    "    data = read_in_data(data_file)\n",
    "    model = read_in_model_code(model_file)\n",
    "    sm = initialize_model(model)\n",
    "    fit = run_model(sm, data, n_iter, n_chains, n_warmup)\n",
    "    write_samples_to_csv(fit)\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:67: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "WARNING:root:`dtypes` ignored when `permuted` is False.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Inference for Stan model: anon_model_234a5a24c057848f5f861d67cf76916e.\n",
       "10 chains, each with iter=5000; warmup=0; thin=1; \n",
       "post-warmup draws per chain=5000, total post-warmup draws=50000.\n",
       "\n",
       "            mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
       "mu          0.04    0.51   1.13  -1.81  -1.04   0.07   1.23   1.68      5    nan\n",
       "tau         1.36    0.61   1.36    0.2   0.42   0.75   1.92   4.75      5    nan\n",
       "sigma       2.74    1.01   2.26   0.15   0.68   1.81   5.71   6.15      5    nan\n",
       "theta[0]   -0.18    0.56   1.26  -1.97  -1.44  -0.07   0.73   1.57      5    nan\n",
       "theta[1]    0.38    0.46   1.03  -1.46  -0.11   0.17    1.1   1.92      5    nan\n",
       "theta[2]   -0.05    0.43   0.96   -1.8  -0.92  -0.06    0.6   1.35      5    nan\n",
       "theta[3]   -0.43    0.46   1.04  -1.94  -1.32  -0.53   0.16   1.56      5    nan\n",
       "log_tau    -0.13    0.42   0.93  -1.59  -0.88  -0.31   0.65   1.56      5    nan\n",
       "log_sigma   0.49    0.52   1.17  -1.89  -0.39   0.58   1.74   1.82      5    nan\n",
       "lp__      -2.6e5   2.9e5  6.4e5 -2.2e6 -1.1e5 -1.6e4  -1564  -1396      5    nan\n",
       "\n",
       "Samples were drawn using NUTS at Mon Oct  2 10:54:07 2017.\n",
       "For each parameter, n_eff is a crude measure of effective sample size,\n",
       "and Rhat is the potential scale reduction factor on split chains (at \n",
       "convergence, Rhat=1)."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main('hierarchical_normal_data.txt', 'hierarchical_model.txt', 5000, 10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
